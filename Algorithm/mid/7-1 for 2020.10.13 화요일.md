7-1

2020.10.13 화요일

#### Master Theorem 1

- Let a, b, and c be nonnegative constants. 
- The solution to the recurrence $$T (1)=1$$, and $$T(n)=aT(n/c)+bn$$, for $$n>1$$for $$n$$ a power of $$ c$$ is
  - 1. $$T(n)=O(n)$$, if $$a<c$$ 
    2. $$T(n)=O(nlogn)$$, if $$a=c$$ 
    3. $$T(n) = O(nlogca)$$, if $$a > c$$
- [Neapolitan 2.8]
- Prove this by induction!
- Avoid divided-and-conquer if, for example–
  - An instance of size n is divided into two or more instances each almost of size n.
  - An instance of size n is divided into almost n instance of size n/c, where c is a constant.

- **The divide-and-conquer strategy often leads to efficient algorithms, although not always!**

#### Master Theorem 2

#### Finding the Closest Pair of 2D Points

**[J. Kleinberg and E. Tardos,** **Algorithm Design****, Addison Wesley, 2005. 5.4]** 

• Problem
 – Given *n* points in the plane, find the pair that is closest together.

• Notation

• Naïve algorithm
 – Compute the distance between each pair of points

and take the minimum → *O*(*n*2) - time

#### Applying the Divide-and-Conquer Strategy [Shamos and Hoey]

- Simple assumption for an easy explanation

  – No two points in *P* have the same *x*-coordinate or the same *y*- coordinate.

- General idea

  [Preprocessing]

  - Build a list *P**x* in which all the points in *P* have been sorted by increasing *x*- coordinate→*O*(*n* log *n*)

  - Build another list *P**y* in which all the points in *P* have been sorted by increasing *y*-coordinate→*O*(*n* log *n*)

    [Recursion for P with |P| = n]

    • [Divide] Partition *P* into two subsets *Q* and *R* → *O*(*n*)
     • [Conquer] Find the closest pairs in *Q* and *R*, respectively→2*T*(*n*/2) • [Combine] Use this information to get the closest pair in *P*→*O*(*n*)

    ✓ Time-complexity
     *O*(*n* log *n*) + *T*(*n*) where *T*(*n*) = *c***n* +2*T*(*n*/2) → **O****(****n** **log** **n****)**

- The stage [Divide]: Partition *P* into two subsets *Q* and *R*. – Create *Q* and *R*, where

  - *Q*: the set of points in the first ceil(*n*/2) positions of the list *P**x* (the “left half”), and

  - *R*: the set of points in the final floor(*n*/2) positions of the list *P**x* (the “right half”).

    – Furthermore, create *Q**x,* *Q**y,* *R**x*, and *R**y*, where

  - *Q**x* consisting of the points in *Q* sorted by increasing *x*-coordinate,

  - *Q**y* consisting of the points in *Q* sorted by increasing *y*-coordinate,

  - *R**x* consisting of the points in *R* sorted by increasing *x*-coordinate, and

  - *R**y* consisting of the points in *R* sorted by increasing *y*-coordinate.

    ✓ Can be done in O(*n*).

- The stage [Conquer]: Find the closest pairs in *Q* and *R*, respectively. – Recursively determine a closest pair (*q*0*, *q*1*) of points in *Q*.

  – Recursively determine a closest pair (*r*0*, *r*1*) of points in *R*. ✓ Can be done in 2*T*(*n*/2).

• The stage [Combine]: Use the obtained info. to get the closest pair in *P*.

- How can we answer this question in linear time?
- – **[Fact 1]** (Why?) •
- – **[Fact 2]** •
- – [Fact 3] •
- Sy : the list consisting of the points in *S* sorted by increasing *y*-coordinate.

- –Each box contains at most one point of *S*. (Why?)

- If two points in *S* are at least 16 positions apart in *S**y* , ...

  – [Merge]

**[Divide] [Conquer]**

**[Combine]**

### [주제 4] Dynamic Programming

#### Algorithm Design Techniques

- Divide-and-Conquer Method
- **Dynamic Programming Method**
- Greedy Method
- Backtracking Method
- Local Search Method
- Branch-and-Bound Method
- Etc.

#### Dynamic Programming: Overview

- From Wikipedia
- Dynamic programming is both a 
  - mathematical optimization method and 
  - a computer programming method.

- A complicated problem is broken down into simpler sub-problems in a recursive manner.
- Overlapping subproblems: A problem is broken down into subproblems which are reused several times or a recursive algorithm for the problem solves the same subproblem over and over rather than always generating new subproblems.
- Optimal substructure: A solution to a given optimization problem can be constructed efficiently from optimal solutions of its subproblems.
- When applicable, the method takes far less time than other methods that don't take advantage of the subproblem overlap like the divide- and-conquer technique.

#### Two Approaches for Recursive Formulation

- Easily becomes exponential!
- Often much more efficient!

#### World Series Odds 

- Problem

- Dodgers and Yankees are playing the World Series in which either team needs to win *n* games first.

- Suppose that each team has a 50% chance of winning any game.

- Let $$P(i,j)$$ be the probability that if Dodgers needs *i* games to win, and

  Yankees needs *j* games, Dodgers will eventually win the Series.

- –  Ex: *P*(2, 3) = 11/16

- –  Compute $$P(i,j) $$  $$ 0 \leq i,j \leq n $$ $$\forall n$$

#### A Divide-and-Conquer Approach 

- Recursive formulation
- If we solve this recurrence relation in the divide-and-conquer way, 
- Let *T*(*n*) be the maximum time taken by a call to *P*(*i*,*j*),where *i*+*j* =*n*. Then we can prove that *T*(*n*) is exponential!
- What is the problem of this approach?

#### A Dynamic Programming Approach 

- Instead of computing the same repeatedly, fill in a table as suggested below:

| 4      | 1    | 15/16 | 13/16 | 21/32 | 1/2   |
| ------ | ---- | ----- | ----- | ----- | ----- |
| 3      | 1    | 7/8   | 11/16 | 1/2   | 11/32 |
| 2      | 1    | 3/4   | 1/2   | 5/16  | 3/16  |
| 1      | 1    | 1/2   | 1/4   | 1/8   | 1/16  |
| 0      |      | 0     | 0     | 0     | 0     |
| *j  i* | 0    | 1     | 2     | 3     | 4     |

- Time Complexity
  -  For input size (m, n), computing P(m, n) takes O(mn)-time.
  - By far better than the Divide-and-Conquer approach.

#### Dynamic Programming

- When the divide-and-conquer approach produces an exponential algorithm where the same sub-problems are solved iteratively,

  1) Take the recursive relation from the divide-and-conquer algorithm, and

  2) replace the recursive calls with table lookups by recording a value in a table entry instead of returning it.

  Top-down → Bottom-up

- Three elements to consider in designing a dynamic programming algorithm

  - Recursive relation
    - Optimal substructure
  - Table setup
  - Table fill order

#### The Manhattan Tourist Problem

- Courtesy of [Jones & Pevzner 6.3]
- Problem:
  - Given two street corners in the borough of Manhattan in New York City, find the path between them with the maximum number of attractions, that is, a path of maximum overall weight.
  - Assume that a tourist may move either to east or to south only.

- A brute force approach
  - Search among all paths in the grid for the longest path!

- A greedy approach
  - 다음 강의 주제

- A formal description of this problem
- Given a weighted graph (grid) G of size (n, m) with two distinguished vertices, a source (0, 0) and a sink (n, m), find a longest path between them in its weighted graph.

**(0, 0)**

- An example grid of size (4, 4)
- A possible selection determined by a greedy approach

- Basic idea
  - How can you use the solutions of smaller problems to build a solution of a problem?
  - A given optimization problem can be constructed efficiently from optimal solutions of its subproblems. →optimal substructure
